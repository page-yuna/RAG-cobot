#rag_final.py
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    pipeline,
)

# 1. ëª¨ë¸ ì„¤ì • (4bit ì–‘ìí™”)
model_id = "mistralai/Mistral-7B-Instruct-v0.3"
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    llm_int8_enable_fp32_cpu_offload=True,
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    device_map="auto",
)

text_generator = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=2048,
    do_sample=True, #ê²½ê³  ë¬´ì‹œí•˜ê¸° ìœ„í•´ trueë¡œ ì„¤ì •
    repetition_penalty=1.03,
    pad_token_id=tokenizer.eos_token_id,
)
llm = HuggingFacePipeline(pipeline=text_generator)

# 2. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
csv_path = "/webcrawling_data.csv"
df = pd.read_csv(csv_path, encoding="ISO-8859-1")

# 3. ë¬¸ì„œ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)
combined_texts = (df["question"].fillna("") + " " + df["answer"].fillna("")).tolist()
docs = text_splitter.create_documents(combined_texts)

# 4. ë²¡í„°ìŠ¤í† ì–´
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = FAISS.from_documents(docs, embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt_template = """
ì œê³µëœ ë‹¤ìŒ ë¬¸ë§¥ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ **í•œêµ­ì–´ë¡œ ëœ ê°„ê²°í•œ ë‹µë³€**ë§Œ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë‹¤ë¥¸ ì •ë³´ë‚˜ ì„œë¡ , ì¶”ê°€ ì„¤ëª…ì€ ì¼ì ˆ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. ë‹µë³€ì€ ì˜¤ì§ í•œêµ­ì–´ë¡œë§Œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

ë¬¸ë§¥:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

ë‹µë³€:
"""
PROMPT = PromptTemplate(template=prompt_template, input_variables=["context", "question"])

# 6. QA ì²´ì¸ êµ¬ì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    input_key="question",
    chain_type_kwargs={"prompt": PROMPT},
)

# 7. ì§ˆì˜ ì‹¤í–‰
if __name__ == "__main__":
    while True:
        user_query = input("\nâ“ ì§ˆë¬¸ ì…ë ¥ (ì¢…ë£Œ: exit): ")
        if user_query.strip().lower() == "exit":
            print("\nğŸ”š ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        result = qa_chain.invoke({"question": user_query})
        response_text = result['result']

        if "ë‹µë³€:" in response_text:
            cleaned = response_text.split("ë‹µë³€:", 1)[1].strip()
        else:
            cleaned = response_text.strip()

        print("\nğŸŸ¢ ë‹µë³€:", cleaned)
